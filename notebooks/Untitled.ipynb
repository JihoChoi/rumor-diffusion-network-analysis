{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../scripts')\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CascadeAnalyzer(object):\n",
    "    feature_df = pd.DataFrame()  # output\n",
    "\n",
    "    def __init__(self):\n",
    "        self.meta_df = pd.DataFrame()  # labels / key: root_tweet_id\n",
    "        self.cascades_dict = {}  # key: root_tweet_id, value: Cascade()\n",
    "        self.retrieve_cascade_labels()\n",
    "        self.load_cascades()\n",
    "\n",
    "    def retrieve_cascade_labels(self):\n",
    "        column_names = ['label', 'tweet_id']\n",
    "        self.meta_df = pd.read_csv(DATA_PATH + \"label.txt\", sep=':', names=column_names, converters={'tweet_id': str})\n",
    "        print(\"-------------------------------------\")\n",
    "        print(self.meta_df.info())\n",
    "        print(\"-------------------------------------\" * 2)\n",
    "        print(self.meta_df.shape, self.meta_df['label'].value_counts().to_dict())\n",
    "        print(\"-------------------------------------\" * 2)\n",
    "        print(self.meta_df.head())\n",
    "        print(\"-------------------------------------\\n\")\n",
    "\n",
    "    def load_cascades(self):\n",
    "        # TODO: handle pickle data\n",
    "        # iterate tweet trees\n",
    "        for index, file in enumerate(os.listdir(DATA_PATH + 'tree_u')):\n",
    "            if not file.endswith('.txt'):\n",
    "                print(\"Unexpected Input File:\", file)\n",
    "                continue\n",
    "            root_tweet_id = file.replace('.txt', '')  # file_id\n",
    "            cascade_path = os.path.join(DATA_PATH + 'tree_u', file)\n",
    "            label = self.meta_df.loc[self.meta_df['tweet_id'] == root_tweet_id, 'label'].item()  # label\n",
    "            self.cascades_dict[root_tweet_id] = Cascade(root_tweet_id, cascade_path, label)\n",
    "            print(self.cascades_dict[root_tweet_id])\n",
    "\n",
    "    # Main Outer loop\n",
    "    def iterate_cascades(self):\n",
    "        for index, row in self.meta_df.iterrows():\n",
    "            tweet_id = row['tweet_id']\n",
    "            cascade = self.cascades_dict[tweet_id]\n",
    "            print('#', index, row['tweet_id'], row['label'])\n",
    "            cascade.calc_structural_features()\n",
    "\n",
    "    def cascade_to_csv(self):  # CascadeAnalyzer\n",
    "        ensure_directory(OUT_PATH)\n",
    "        out_file_name = OUT_PATH + 'structural_analysis_' + time.strftime(\"%Y%m%d_%H%M%S\") + \".csv\"\n",
    "        out_file = open(out_file_name, 'w', encoding='utf-8', newline='')\n",
    "        self.feature_df.to_csv(out_file, sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cascade:\n",
    "\n",
    "    # --------------------------\n",
    "    #      Initiate Cascade\n",
    "    # --------------------------\n",
    "    def __init__(self, root_tweet_id, cascade_path, label=None):\n",
    "        self.file_id = root_tweet_id  # For label.txt\n",
    "        self.root_tweet_id = root_tweet_id  # Tweet ID with ROOT Keyword (May updated)\n",
    "        self.root_user_id = 0\n",
    "        self.cascade_path = cascade_path\n",
    "        self.label = label\n",
    "\n",
    "        # ------------\n",
    "        # Load Cascade\n",
    "        # ------------\n",
    "        self.trace_count = None\n",
    "        self.src_users = set()\n",
    "        self.dst_users = set()\n",
    "        self.retweet_users = set()\n",
    "        self.reply_users = set()\n",
    "        self.retweet_count = 0\n",
    "        self.reply_count = 0\n",
    "        self.network = nx.DiGraph()\n",
    "        self.network_features = {}\n",
    "        self.load_cascade()\n",
    "\n",
    "        # -----------------\n",
    "        # Calculate Cascade\n",
    "        # -----------------\n",
    "        self.src_user_count = None\n",
    "        self.dst_user_count = None\n",
    "        self.avg_depth = 0\n",
    "        self.max_depth = 0\n",
    "\n",
    "    def load_cascade(self):\n",
    "        with open(self.cascade_path, 'r') as file:\n",
    "            # ---- -----------------\n",
    "            # Set Root: User, Tweet\n",
    "            # ---------------------\n",
    "            for index, line in enumerate(file):\n",
    "                elem_list = [x.strip() for x in re.split(r\"[\\'\\,\\->\\[\\]]\", line.strip()) if x.strip()]\n",
    "                if elem_list[0] == 'ROOT' and elem_list[1] == 'ROOT':\n",
    "                    self.root_user_id = elem_list[3]\n",
    "                    if index != 0:\n",
    "                        print('ROOT TWEET {} by {} @ line # {}'.format(elem_list[4], self.root_user_id, index))\n",
    "                    break\n",
    "            if self.root_tweet_id != elem_list[4]:  # Assert file_id == root_tweet_id\n",
    "                print('\\t file_id:{1} -> root_tweet_id:{2} ({0}) '.format(self.label, self.root_tweet_id, elem_list[4]))\n",
    "                self.root_tweet_id = elem_list[4]\n",
    "            # ------------\n",
    "            # Load Cascade\n",
    "            # ------------\n",
    "            for index, line in enumerate(file):  # Trace\n",
    "                elem_list = re.split(r\"[\\'\\,\\->\\[\\]]\", line.strip())\n",
    "                elem_list = [x.strip() for x in elem_list if x.strip()]  # Remove empty elements\n",
    "                # Error data handling\n",
    "                if float(elem_list[2]) >= float(elem_list[5]):\n",
    "                    continue\n",
    "                src_user_id, src_tweet_id, src_tweet_time, dst_user_id, dst_tweet_id, dst_tweet_time = elem_list\n",
    "                self.src_users.add(src_user_id)\n",
    "                self.dst_users.add(dst_user_id)\n",
    "                # Different types of Tweets - https://help.twitter.com/en/using-twitter/types-of-tweets\n",
    "                if src_tweet_id == dst_tweet_id:\n",
    "                    self.retweet_count += 1\n",
    "                    self.retweet_users.add(dst_user_id)\n",
    "                else:\n",
    "                    self.reply_count += 1\n",
    "                    self.reply_users.add(dst_user_id)\n",
    "                # NetworkX Graph\n",
    "                self.network.add_weighted_edges_from(\n",
    "                    [(src_user_id, dst_user_id, float(dst_tweet_time) - float(src_tweet_time))])\n",
    "        # Store computed cascade information\n",
    "        self.trace_count = index\n",
    "\n",
    "    # =============================\n",
    "    #      Structural Analysis\n",
    "    # =============================\n",
    "    def calc_structural_features(self):\n",
    "        G = self.network\n",
    "        # root_user_id = self.root_user_id\n",
    "        self.src_user_count = len(self.src_users)\n",
    "        self.dst_user_count = len(self.dst_users)\n",
    "        hops = []\n",
    "        max_hop_count = 10\n",
    "        for i in range(max_hop_count):\n",
    "            hops.append(len(nx.single_source_shortest_path_length(G, self.root_user_id, cutoff=i)))\n",
    "\n",
    "        # print(self.retweet_count, self.response_count)\n",
    "        # print(\"leaf:\", nx.dag_to_branching(G))\n",
    "        # print('\\t root_to_all_depth_length: ', len(nx.single_source_shortest_path_length(G, self.root_user_id)))\n",
    "        # print('\\t user_count:', len(G.nodes()))  # root + dst_user_count\n",
    "        print('\\t depth: ', nx.dag_longest_path(G))  # weight - temporal feature\n",
    "        print('\\t src_user_count: ', self.src_user_count)\n",
    "        print('\\t dst_user_count: ', self.dst_user_count)\n",
    "        print('\\t root_to_all_depth_sum: ', sum(nx.single_source_shortest_path_length(G, self.root_user_id).values()))\n",
    "        print('\\t root_to_all_depth_max: ', max(nx.single_source_shortest_path_length(G, self.root_user_id).values()))\n",
    "        print('\\t one_hop_neighbors:', len(list(G.neighbors(self.root_user_id))))\n",
    "        print('\\t', \"user count by hop(s): \", hops[1] - hops[0], hops[2] - hops[1], hops[3] - hops[2],\n",
    "                                              hops[4] - hops[3], hops[5] - hops[4], hops[6] - hops[5],\n",
    "                                              hops[7] - hops[6], hops[8] - hops[7], hops[9] - hops[8])\n",
    "\n",
    "        # df.loc[df['tweet_id'] == root_tweet_id, 'src_user_count'] = len(src_users)\n",
    "        shortest_path_dict = nx.single_source_shortest_path_length(G, self.root_user_id)\n",
    "        self.avg_depth = sum(shortest_path_dict.values()) / len(shortest_path_dict)\n",
    "        self.max_depth = max(shortest_path_dict.values())\n",
    "\n",
    "        for i in range(max_hop_count - 1):\n",
    "            self.network_features[str(i) + \"_hop_neighbor_count\"] = hops[i + 1] - hops[i]\n",
    "\n",
    "\n",
    "\n",
    "        # features to data frame\n",
    "        CascadeAnalyzer.feature_df = CascadeAnalyzer.feature_df.append({\n",
    "            'tweet_id': self.root_tweet_id, 'label': self.label,\n",
    "            'structural_trace_count': self.trace_count,\n",
    "            'structural_retweet_count': self.retweet_count,\n",
    "            'structural_reply_count': self.reply_count,\n",
    "            'structural_src_user_count': self.src_user_count,\n",
    "            'structural_dst_user_count': self.dst_user_count,\n",
    "            'structural_retweet_reply_percent': self.retweet_count / (self.retweet_count + self.reply_count),\n",
    "            'structural_src_dst_user_percent': self.src_user_count / (self.src_user_count + self.dst_user_count),  # <--\n",
    "            'structural_retweet_users_count': len(self.retweet_users),\n",
    "            'structural_reply_users_count': len(self.reply_users),\n",
    "            'structural_root_to_all_depth_sum': sum(nx.single_source_shortest_path_length(G, self.root_user_id).values()),\n",
    "            'structural_root_to_all_depth_max': max(nx.single_source_shortest_path_length(G, self.root_user_id).values()),\n",
    "            'structural_1_hop_neighbor_count': self.network_features['1_hop_neighbor_count'],\n",
    "            'structural_2_hop_neighbor_count': self.network_features['2_hop_neighbor_count'],\n",
    "            'structural_3_hop_neighbor_count': self.network_features['3_hop_neighbor_count'],\n",
    "            'structural_4_hop_neighbor_count': self.network_features['4_hop_neighbor_count'],\n",
    "            'structural_5_hop_neighbor_count': self.network_features['5_hop_neighbor_count'],\n",
    "            'structural_6_hop_neighbor_count': self.network_features['6_hop_neighbor_count'],\n",
    "            'structural_7_hop_neighbor_count': self.network_features['7_hop_neighbor_count'],\n",
    "            'structural_8_hop_neighbor_count': self.network_features['8_hop_neighbor_count'],\n",
    "            'structural_avg_depth': self.avg_depth,\n",
    "            'structural_max_depth': self.max_depth,  # duplicate\n",
    "            # 'structural_max_depth': self.max_depth,  # duplicate\n",
    "            'structural_network_density': nx.density(G),  # duplicate\n",
    "        }, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = CascadeAnalyzer()\n",
    "analyzer.iterate_cascades()\n",
    "analyzer.cascade_to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer.cascades_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = analyzer.cascades_dict['498430783699554305'].network\n",
    "G2 = analyzer.cascades_dict['673615400655970304'].network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(list(nx.betweenness_centrality(G).values()))  # average betweenness centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(list(nx.betweenness_centrality(G2).values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer.cascades_dict['498430783699554305'].root_user_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(nx.single_source_shortest_path_length(G, analyzer.cascades_dict['498430783699554305'].root_user_id).values()) / analyzer.cascades_dict['498430783699554305'].dst_user_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(nx.single_source_shortest_path_length(G2, analyzer.cascades_dict['673615400655970304'].root_user_id).values()) / analyzer.cascades_dict['673615400655970304'].dst_user_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
